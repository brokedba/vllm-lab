# vLLM Lab ğŸš€

## Overview ğŸ“Œ

This repository contains Terraform configurations for deploying [vLLM](https://github.com/vllm-project/vllm) on Kubernetes clusters across multiple cloud providers, including Civo, Oracle Cloud (OCI), AWS, GCP, and Azure. This setup is based on the [llmcache vllm production-stack](https://github.com/llmcache/vllm-production-stack) project.

> [!TIP]
> For non Kubernetes deployments including CPU builds please check my [Installation.tutorial](./docs/Installation.md)

## Supported Platforms ğŸŒ

- **Civo Kubernetes** â˜ï¸
- **Oracle Cloud (OCI) OKE** ğŸ›ï¸
- **Amazon Web Services (AWS) EKS** ğŸŸ 
- **Google Cloud Platform (GCP) GKE** ğŸ”µ
- **Microsoft Azure AKS** ğŸ”·

## VLLM Features âœ¨
- **PagedAttention ğŸš€**: Enables efficient memory management to handle large context sizes, reducing redundant computation and maximizing GPU utilization.
- **Continuous Batching ğŸ“¦**: Dynamically schedules incoming requests for maximum throughput.
- **Multi-GPU and Distributed Support**: Seamless scaling across multiple GPUs and nodes.
- **Fast Token Generation âš¡**: Optimized kernel implementation speeds up inference performance.
- **Flexible Deployment**: Works with Kubernetes, Docker, and cloud-based orchestration tools.
- **Multi-Backend Support ğŸ”—**: Works across different cloud providers and Kubernetes environments.

# Production stack features
- **Interface Design:** Production Stack and LMCache is built as an open, extensible framework, intentionally leaving room for community contributions and innovations. Keeping the interface flexible to support more storage and compute devices in the future.
- **vLLM Support:** LMCache supports the latest vLLM versions through the KV connector interface ([PR](https://github.com/vllm-project/vllm/pull/12953)) and will continue to contribute and support the latest vLLM by leveraging an vLLM upstream connector.
- **KV Cache Performance:** LMCache has advanced KV cache optimizationsâ€”efficient **KV transfer and blending**, particularly useful for long-context inference.
- **Developer Friendliness:** In Production Stack and LMCache, operators can directly program LLM serving logics in Python, allowing more optimizations in the long run. Production stack is also easy to setup in 5 minutes [here ](https://github.com/vllm-project/production-stack/tree/main/tutorials).
## Deployment Guide ğŸ› ï¸

### Prerequisites âœ…

Ensure you have the following installed:

- [Terraform](https://developer.hashicorp.com/terraform/downloads) ğŸŒ
- [kubectl](https://kubernetes.io/docs/tasks/tools/) â›µ
- Cloud CLI tools:
  - [Civo CLI](https://www.civo.com/docs) âš¡
  - [OCI CLI](https://docs.oracle.com/en-us/iaas/Content/API/SDKDocs/cli.htm) â˜ï¸
  - [AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html) ğŸŸ 
  - [gcloud CLI](https://cloud.google.com/sdk/docs/install) ğŸ”µ
  - [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) ğŸ”·

### Setup Instructions ğŸ“–

Each cloud provider has its own Terraform configuration. Follow these steps:

#### 1. Clone the Repository ğŸ—ï¸

```bash
git clone https://github.com/brokedba/vllm_lab.git
cd vllm_lab
```

#### 2. Initialize Terraform âš™ï¸

Navigate to the specific cloud provider directory and initialize Terraform:

```bash
cd terraform/<provider>
terraform init
```

#### 3. Customize Variables âœï¸

Update the `terraform.tfvars` file with your credentials and configuration options.

#### 4. Deploy the Cluster ğŸš€

```bash
terraform apply -auto-approve
```

#### 5. Deploy vLLM ğŸ§ 

Once the Kubernetes cluster is up, deploy `vLLM` using Helm:

```bash
helm repo add vllm https://vllm-project.github.io/helm-charts/
helm install vllm vllm/vllm --namespace vllm --create-namespace
```

## Structure ğŸ“‚

```
vllm-lab/
â”œâ”€â”€ terraform/
â”‚   â”œâ”€â”€ civo/
â”‚   â”œâ”€â”€ oci/
â”‚   â”œâ”€â”€ aws/
â”‚   â”œâ”€â”€ gcp/
â”‚   â”œâ”€â”€ azure/
â”œâ”€â”€ helm/
â”‚   â”œâ”€â”€ values.yaml
â”‚   â”œâ”€â”€ templates/
â””â”€â”€ README.md
```

## Next Steps â­ï¸

- Implement monitoring with Prometheus/Grafana ğŸ“Š
- Add autoscaling configurations for vLLM ğŸ“ˆ
- Improve security configurations ğŸ”’

## Contributions ğŸ¤

PRs and issues are welcome! ğŸš€


servingEngineSpec:  
  enableEngine: true  
  runtimeClassName: ""  # Use default runtime for CPU 
  containerSecurityContext:  
    privileged: true  
  modelSpec:  
  - name: "tinyllama-cpu"  
    repository: "public.ecr.aws/q9t5s3a7/vllm-cpu-release-repo"  
    tag: "v0.8.5.post1"  
    modelURL: "TinyLlama/TinyLlama-1.1B-Chat-v1.0"  
    replicaCount: 1  
    requestCPU: 3  
    requestMemory: "3Gi"  
    requestGPU: 0  # CPU Only
    limitCPU: "4"  
    limitMemory: "6Gi"  
    pvcStorage: "10Gi"  
 #  device: "cpu"   Undocumented    
    vllmConfig:  
      dtype: "bfloat16"  
      extraArgs:  
        - "--device"  
        - "cpu"   
    env:  
      - name: VLLM_CPU_KVCACHE_SPACE  
        value: "1"  
      - name: VLLM_CPU_OMP_THREADS_BIND  
        value: "0-2"  
      - name: HUGGING_FACE_HUB_TOKEN  # Optional for TinyLlama
        valueFrom:  
          secretKeyRef:  
            name: hf-token-secret  
            key: token    
routerSpec:  
  enableRouter: true  
  routingLogic: "roundrobin"
